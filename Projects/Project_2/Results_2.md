# Core Parts

## Tune the algorithm for accuracy of classification. Give a table or graph of the percentage of correctly classified digits as a function of the number of basis vectors. 

![Graph of Accuracies](Screenshots\graph_of_accuracies.PNG)

## Check if all digits are equally easy or difficult to classify. Also look at some of the difficult ones and see that in many cases they are very badly written.

We see if all the digits are equally easy or difficult to classify using a classification report:

![Classification report of the main algorithm](Screenshots\classification_report.PNG)

Then, we plot some of the poorly written ones which ended up being misclassified:

![Misclassified Numbers](Screenshots\misclassified.PNG)

## Check the singular values of the different classes. Is it motivated to use different numbers of basis vectors for different classes? If so, perform a few experiments to find out if it really pays off to use fewer basis vectors in one or two of the classes. 

We first check the singular values of the different classes by plotting them. 
Note: The matrix S containing the singular values is cut at 20 basis vectors.

![Singular Values](Screenshots\singular_values.PNG)

Then, instead of running the algorithm again for some specific digits to see whether the basis vectors will be 18 like in the general case, we use the matrix we generated by running the first algorithm. We view this matrix horizontally instead of vertically and find effectively the optimal number of basis vectors for each digit, for the test dataset.

# Bonus --> Two-Stage SVD

## In order to save operations in the test phase, implement a two-stage algorithm: In the first stage compare the unknown digit only to the first singular vector in each class. If for one class the residual is significantly smaller than for the others, classify as that class. Otherwise perform the algorithm above. Is it possible to get as good results for this variant? How frequently is the second stage unnecessary?

We implement the algorithm with a 0.9 as a threshold. This is set because we "agree" to sacrifice up to 1% accuracy. We can't get as good results for this variant, but we can get close enough results while saving a significant amount of operations. Then we run the algorithm and we get the graph of accuracies as well as the classification report. The results are expected, as we sacrificed a bit of accuracy to save operations. Then, we calculate the relative ratio and see that this alternative approach saves us a lot of time as it is (usually) more than x2 times faster. Also, the 2nd phase using 0.9 as a threshold was unnecessary in 57% of the data rows.